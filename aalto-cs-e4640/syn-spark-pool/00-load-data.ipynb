{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspaalto01",
              "session_id": "0",
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-03-15T21:38:38.1566395Z",
              "session_start_time": "2023-03-15T21:38:39.2469713Z",
              "execution_start_time": "2023-03-15T21:43:09.2613609Z",
              "execution_finish_time": "2023-03-15T21:43:09.4499843Z",
              "spark_jobs": null,
              "parent_msg_id": "c247690c-3b52-4932-809e-d51945a01131"
            },
            "text/plain": "StatementMeta(synspaalto01, 0, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'3.1.3.5.0-83093024'"
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "spark.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspaalto01",
              "session_id": "0",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-03-15T21:45:32.3930997Z",
              "session_start_time": null,
              "execution_start_time": "2023-03-15T21:45:32.5979999Z",
              "execution_finish_time": "2023-03-15T21:47:52.3794071Z",
              "spark_jobs": null,
              "parent_msg_id": "f4c13910-03e4-48ff-b6fb-e7e19e21609b"
            },
            "text/plain": "StatementMeta(synspaalto01, 0, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copying full data for folder: dimension_city\ncopied full data for folder: dimension_city\ncopying full data for folder: dimension_customer\ncopied full data for folder: dimension_customer\ncopying full data for folder: dimension_date\ncopied full data for folder: dimension_date\ncopying full data for folder: dimension_employee\ncopied full data for folder: dimension_employee\ncopying full data for folder: dimension_stock_item\ncopied full data for folder: dimension_stock_item\ncopying full data for folder: fact_sale_1y_full\ncopied full data for folder: fact_sale_1y_full\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "source_wasb = \"wasbs://sampledata@azuresynapsestorage.blob.core.windows.net/WideWorldImportersDW\"\r\n",
        "blob_relative_path = \"WideWorldImportersDW\"\r\n",
        "\r\n",
        "folders = mssparkutils.fs.ls(source_wasb + \"/parquet/full/\")\r\n",
        "folders.sort(key=lambda x: x.name)\r\n",
        "for folder in folders:\r\n",
        "  if folder.isDir:\r\n",
        "      source_folder = source_wasb + \"/parquet/full/\" + folder.name\r\n",
        "      target_csv_folder = \"/csv/full/\" + folder.name\r\n",
        "      if \"fact_sale_4y_full\" not in folder.name: \r\n",
        "          print(\"copying full data for folder: \" + folder.name) \r\n",
        "          df = spark.read.format(\"parquet\").load(source_folder)\r\n",
        "          df.write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save('abfss://default@staaltolab01.dfs.core.windows.net/aalto/bronze' + target_csv_folder)\r\n",
        "          print(\"copied full data for folder: \" + folder.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synspaalto01",
              "session_id": "0",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-03-15T21:47:59.6955625Z",
              "session_start_time": null,
              "execution_start_time": "2023-03-15T21:47:59.8733119Z",
              "execution_finish_time": "2023-03-15T21:48:31.3493081Z",
              "spark_jobs": null,
              "parent_msg_id": "e7a60f53-8dc6-4472-a53a-24dec738e470"
            },
            "text/plain": "StatementMeta(synspaalto01, 0, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copying incremental data for folder: fact_sale_1y_incremental\ncopied incremental data for folder: fact_sale_1y_incremental\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "source_wasb = \"wasbs://sampledata@azuresynapsestorage.blob.core.windows.net/WideWorldImportersDW\"\r\n",
        "blob_relative_path = \"WideWorldImportersDW\"\r\n",
        "\r\n",
        "folders = mssparkutils.fs.ls(source_wasb + \"/parquet/incremental/\")\r\n",
        "folders.sort(key=lambda x: x.name)\r\n",
        "for folder in folders:\r\n",
        "  if folder.isDir:\r\n",
        "      source_folder = source_wasb + \"/parquet/incremental/\" + folder.name\r\n",
        "      target_csv_folder = \"/csv/incremental/\" + folder.name\r\n",
        "      if \"fact_sale_4y_incremental\" not in folder.name: \r\n",
        "          print(\"copying incremental data for folder: \" + folder.name)\r\n",
        "          df = spark.read.format(\"parquet\").load(source_folder)\r\n",
        "          df.write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save('abfss://default@staaltolab01.dfs.core.windows.net/aalto/bronze' + target_csv_folder) \r\n",
        "          print(\"copied incremental data for folder: \" + folder.name)"
      ]
    }
  ]
}